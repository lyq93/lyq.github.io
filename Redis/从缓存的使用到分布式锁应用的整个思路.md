### 从缓存的使用到分布式锁应用的整个思路

### 为什么使用缓存

> 原因：提高系统吞吐量，避免部分请求落到数据库中
>
> 所谓的部分请求可以分析为如下两种类型：
>
> 1、频繁查询的请求且数据较为稳定（例如字典数据、分类菜单等基础聚合数据）
>
> 2、准确性要求没那么高的大聚合数据（一次请求多次查库且需要数据组装返回）

### 最初的缓存实现

> 最初的缓存实现通过使用Map该数据结构就可以实现缓存功能。
>
> 但是存在问题是，在分布式环境下，通过Map实现的缓存是进程内的。随着业务量的提升，一个服务可能存在多个实例（多个JVM进程），这个时候相当于每个JVM进程都各自维护着自己的一套缓存数据，这个时候就可能存在如下问题：
>
> 1、通过nginx负载均衡过来的请求，使得每台服务至少会有一次对数据库的请求
>
> 2、服务之间的缓存数据会不一致，如果落在服务器A的请求修改了缓存，其他服务器的缓存并没有更新，导致缓存数据不一致
>
> 由此，淘汰用Map实现的缓存，转而通过考虑把缓存从JVM进程中抽离出来，让多个进程共享一份缓存数据解决相应问题。这个的落地产品就是redis

### 缓存的进阶实现

> 首先，redis实现缓存解决了本地缓存的问题，但依然存在使用缓存所带来的其他问题：
>
> 1、缓存穿透：大量请求的数据不在缓存中并且极端情况下可以利用这一特点进行恶意请求，导致数据库崩溃
>
> 一个简单的处理方式是直接把null值也缓存起来，设置一个过期时间。另外一种实现方式是布隆过滤器
>
> 2、缓存雪崩：缓存可能在同一时间点大批量的失效，导致某一时刻大批量的数据落到数据库。出现这个问题大概率是在缓存数据的时候设置的过期时间没有采用随机时间
>
> 简单的处理方式是缓存数据的时候，对于过期时间采用随机的方式生成
>
> 3、缓存击穿：假设，针对某一个key可能同一时刻有100w的请求并发量，那么在这个时刻刚好该key失效了，导致这个100w请求会直接穿过缓存落到数据库中
>
> 针对这个问题，那么需要保证这100w的请求只有1个请求会实际落到数据库中，其他请求需要阻塞住。那么也就是说，当穿过缓存之后，需要对查库逻辑进行加锁处理，1个请求完成操作库的逻辑，其他请求走缓存。这个相当于是单例模式，所以使用的是双重校验来实现的。这里就基本上解决了缓存击穿带来的大并发量的请求落到库里的情况
>
> 另外，这里使用到了锁，不管是使用sync对象还是JUC下的锁，都是在JVM进程内部的锁，这个问题又回到了分布式服务下跟本地缓存面临的一摸一样的问题。
>
> 这个时候就需要把锁也要抽离出来，如此就需要使用分布式锁